---
title: "Navigating the S&P 500 with Machine Learning"
subtitle: "INFO 523 - Summer 2025 - Final Project"
author: "Trevor Macdonald & Nandakumar Kuthalaraja"
title-slide-attributes:
  data-background-image: images/sp500.png
  data-background-size: cover
  data-background-opacity: ".3"
  data-slide-number: none
  style: "color: black;"
format:
  revealjs:
    theme:  ['data/customtheming.scss']
    css: images/style.css
  
editor: visual
jupyter: python3
execute:
  echo: false
---

## What is the S&P 500?

::: smaller
-   The **S&P 500 (Standard & Poor’s 500 Index)** is a stock market index that tracks the performance of **500 of the largest publicly traded companies** in the United States.\
-   It is considered a **benchmark** for the overall U.S. stock market and economy.\
-   The index covers companies across **11 sectors**, including technology, healthcare, finance, and consumer goods.
-   Investors, analysts, and researchers use the S&P 500 to:
    -   Measure market performance\
    -   Benchmark investments\
    -   Study trends in the U.S. economy\
:::

## Distribution of S&P 500

::: smaller
```{python}
#| label: setup001

import pandas as pd
import matplotlib.pyplot as plt

# 1) Load current S&P 500 constituents with sectors from Wikipedia
url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
dfs = pd.read_html(url, match="Symbol")
df = dfs[0].copy()

# 2) Normalize column names and get sector column robustly
df.columns = [c.strip() for c in df.columns]
sector_col = [c for c in df.columns if c.lower().startswith("gics sector")][0]

# 3) Count & percentage by sector
counts = df[sector_col].value_counts().sort_values(ascending=True)
pct = (counts / counts.sum() * 100).round(1)

# 4) Plot (horizontal bar chart with labels)
fig, ax = plt.subplots(figsize=(10,6), dpi=150)
ax.barh(counts.index, pct.values)

for i, (v) in enumerate(pct.values):
    ax.text(v, i, f"  {v:.1f}%", va="center")

ax.set_xlabel("Percent of Companies")
ax.set_ylabel("")
ax.set_title("S&P 500 Companies by Sector (%)", pad=12)
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)
plt.tight_layout()
plt.show()

```
:::

## Our Research Questions (Overview)

::: smaller
-   **Q1:** Classify each 2024 trading day into **low / medium / high volatility** from recent price action & indicators.
-   **Q2:** Use **hierarchical clustering** to build a taxonomy of S&P 500 stocks from **multi-factor risk/return** features for 2014-2024.
-   **Q3:** Can a Long Short-Term Memory **LSTM** model accurately forecast short, medium, or long term S&P 500 returns?
-   **Q4:** How does forecast accuracy degrade as a function of prediction horizon, and what does this suggest about LSTM’s ability to model longer term financial trends?
:::

## Q1 — Volatility Regime Classification

::: smallestest
**Goal:** Classify each 2024 trading day into **Low / Medium / High** volatility regimes

**Why it matters:** Regime awareness supports **risk control, hedging, and position sizing**

**Features (look-back window):** - Realized volatility (20d rolling std of returns), intraday range\
- Momentum & trend (MA ratios, RSI)\
- Volume dynamics (volume/MA ratios)\
- Market sentiment (VIX)

**Labels:** **Tertiles of realized volatility** → Low / Medium / High

**Models Tested:** Random Forest

**Evaluation:** Macro-F1, balanced accuracy, confusion matrix
:::

## Q1 \| Step 1 — Feature Engineering

::: tiny
We construct key price-based and market indicators:

-   **20-day rolling volatility**\
-   **Intraday range** (High – Low) / Close\
-   **Moving averages** (MA10 / MA50 ratio)\
-   **Volume ratios** (Volume / MA20)\
-   **RSI** (14-day)\
-   **VIX index** (implied volatility)

These features summarize short-term market dynamics and investor sentiment.
:::

```{python}
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# helper to normalize columns
def normalize_columns(df):
    if isinstance(df.columns, pd.MultiIndex):
        df = df.copy()
        df.columns = df.columns.get_level_values(0)
    else:
        # handle stringified tuples like "('Close','^GSPC')"
        import ast
        new_cols = []
        for c in df.columns:
            s = str(c)
            if s.startswith("(") and "," in s:
                try:
                    t = ast.literal_eval(s)
                    new_cols.append(t[0])
                except Exception:
                    new_cols.append(s)
            else:
                new_cols.append(s)
        df = df.copy()
        df.columns = new_cols
    return df


data = yf.download('^GSPC', start='2014-01-01', end='2024-12-31', auto_adjust=False)
data = normalize_columns(data)


vix = yf.download('^VIX', start='2014-01-01', end='2024-12-31', auto_adjust=False)
vix = normalize_columns(vix)
vix = vix[['Close']].rename(columns={'Close':'VIX'})

# normalize indexes
data.index = pd.to_datetime(data.index).tz_localize(None)
vix.index   = pd.to_datetime(vix.index).tz_localize(None)

# join safely
data = data.join(vix, how='left')
data['VIX'] = data['VIX'].ffill()

# features needed for the plot
data['Return'] = data['Close'].pct_change()
data['Volatility20'] = data['Return'].rolling(20, min_periods=20).std() * np.sqrt(252)
# Intraday range
data['Range'] = (data['High'] - data['Low']) / data['Close']

# Moving averages + ratio
data['MA10'] = data['Close'].rolling(10, min_periods=10).mean()
data['MA50'] = data['Close'].rolling(50, min_periods=50).mean()
data['MA_ratio'] = data['MA10'] / data['MA50']

# Volume ratios
data['Vol_MA20'] = data['Volume'].rolling(20, min_periods=5).mean()
data['Vol_ratio'] = data['Volume'] / data['Vol_MA20']

# RSI (14)
delta = data['Close'].diff()
up = delta.clip(lower=0)
down = (-delta).clip(lower=0)
roll_up = up.rolling(14, min_periods=14).mean()
roll_down = down.rolling(14, min_periods=14).mean().replace(0, np.nan)
RS = roll_up / roll_down
data['RSI'] = 100 - (100 / (1 + RS))


cols = ['Volatility20','VIX','Range','MA_ratio','Vol_ratio','RSI']
f = data.loc['2024', cols].dropna(how='all').copy()

# Build small multiples
fig, axes = plt.subplots(2, 3, figsize=(14,6), dpi=150, sharex=True)
axes = axes.ravel()

series_info = [
    ('Volatility20', '20d Realized Vol (annualized)'),
    ('VIX',          'VIX (implied vol)'),
    ('Range',        'Intraday Range (H-L)/Close'),
    ('MA_ratio',     'MA10 / MA50'),
    ('Vol_ratio',    'Volume / MA20'),
    ('RSI',          'RSI (14)')
]

for ax, (col, title) in zip(axes, series_info):
    if col in f.columns:
        ax.plot(f.index, f[col], linewidth=1.2)
        ax.set_title(title)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)

# Tidy layout
for i in (3,4,5):
    axes[i].set_xlabel("Date")
plt.tight_layout()
plt.show()


```

## Q1 \| Step 2 — Target Creation

::: tiny
We label each 2024 trading day as **Low / Medium / High** using tertiles of realized volatility, and visualize the assignments across the year.
:::

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Guard: create targets if not already present 
if 'df_2024' not in globals() or 'Vol_Regime' not in df_2024.columns:
    # Build from Step 1 outputs
    data['Return'] = data['Close'].pct_change()
    data['Volatility20'] = data['Return'].rolling(20, min_periods=20).std() * np.sqrt(252)
    df_2024 = data.loc['2024'].dropna(subset=['Volatility20']).copy()
    low_q, high_q = df_2024['Return'].abs().quantile([0.33, 0.66])
    df_2024['Vol_Regime'] = pd.cut(
        df_2024['Return'].abs(),
        bins=[-np.inf, low_q, high_q, np.inf],
        labels=['Low','Medium','High']
    )

# Colors for regimes 
colors = {"Low": "green", "Medium": "orange", "High": "red"}

# Create combined figure 
fig, axes = plt.subplots(1, 2, figsize=(14,5), dpi=150)

# Volatility timeline with regimes
ax = axes[0]
for regime, color in colors.items():
    s = df_2024[df_2024['Vol_Regime'] == regime]
    ax.scatter(s.index, s['Volatility20'], s=15, alpha=0.7, color=color, label=regime)
ax.set_title("Volatility Regimes Across 2024")
ax.set_ylabel("20d Realized Vol (annualized)")
ax.set_xlabel("Date")

# Closing price with regimes
ax = axes[1]
ax.plot(df_2024.index, df_2024['Close'], color='black', linewidth=1, alpha=0.8)
for regime, color in colors.items():
    mask = df_2024['Vol_Regime'] == regime
    ax.scatter(df_2024.index[mask], df_2024['Close'][mask], s=12, c=color, alpha=0.8)
ax.set_title("S&P 500 Closing Price with Regimes (2024)")
ax.set_xlabel("Date")
ax.set_ylabel("Index Level")

# Shared legend
handles = [plt.Line2D([0],[0], marker='o', color='w', label=k,
                      markerfacecolor=v, markersize=6) for k, v in colors.items()]
fig.legend(handles=handles, labels=list(colors.keys()),
           loc='upper center', ncol=3, frameon=False)

plt.tight_layout(rect=[0,0,1,0.94])
plt.show()

```

:::: smallest
::: callout-note
Balanced Labels — Days are evenly split into Low, Medium, High volatility using realized vol tertiles.

Temporal Patterns — High-volatility clusters appear during market drawdowns, while low regimes align with calm uptrends.

Practical Use — These labels provide the supervised learning target for Step 3 (model training).
:::
::::

## Q1 \| Step 3 — Model Training

::: tiny
We train a **Random Forest classifier** on engineered features to predict volatility regimes.\
Evaluation is done with a confusion matrix and Feature Importance ranking.
:::

:::: output-box
::: tiny
```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report

# Ensure VIX exists (must have been joined in Step 1)
assert 'VIX' in data.columns, "VIX missing on `data`. Run Step 1 cell that joins VIX."

# Recompute features on the full `data` to be safe
data = data.copy()
data['Return'] = data['Close'].pct_change()
data['Volatility20'] = data['Return'].rolling(20, min_periods=20).std() * np.sqrt(252)
data['Range'] = (data['High'] - data['Low']) / data['Close']
data['MA10'] = data['Close'].rolling(10, min_periods=10).mean()
data['MA50'] = data['Close'].rolling(50, min_periods=50).mean()
data['MA_ratio'] = data['MA10'] / data['MA50']
data['Vol_MA20'] = data['Volume'].rolling(20, min_periods=5).mean()
data['Vol_ratio'] = data['Volume'] / data['Vol_MA20']

# Build 2024 frame + target labels
df_2024 = data.loc['2024'].copy()
df_2024['DailyVol'] = df_2024['Return'].abs()

q1, q2 = df_2024['DailyVol'].quantile([0.33, 0.66])
df_2024['Vol_Regime'] = pd.cut(
    df_2024['DailyVol'],
    bins=[-np.inf, q1, q2, np.inf],
    labels=['Low','Medium','High']
)

# Feature matrix 
features = ['Volatility20','Range','MA_ratio','Vol_ratio','RSI','VIX']

# If RSI wasn't computed in this session, create it
if 'RSI' not in df_2024.columns:
    delta = data['Close'].diff()
    up = delta.clip(lower=0)
    down = (-delta).clip(lower=0)
    roll_up = up.rolling(14, min_periods=14).mean()
    roll_down = down.rolling(14, min_periods=14).mean().replace(0, np.nan)
    RS = roll_up / roll_down
    data['RSI'] = 100 - (100 / (1 + RS))
    df_2024['RSI'] = data.loc[df_2024.index, 'RSI']

# Make sure all features exist now
missing = [c for c in features if c not in df_2024.columns]
if missing:
    raise ValueError(f"Missing features in df_2024: {missing}")

# Fill NaNs from rolling windows
df_2024[features] = df_2024[features].bfill().ffill()

# Drop any remaining NaNs along features/target
df_2024 = df_2024.dropna(subset=features + ['Vol_Regime'])

X = df_2024[features]
y = df_2024['Vol_Regime']

# Time-aware split (no shuffling)
split_idx = int(len(df_2024) * 0.75)
X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

# Train model
rf = RandomForestClassifier(n_estimators=300, random_state=42)
rf.fit(X_train, y_train)

# Evaluate
y_pred = rf.predict(X_test)
print(classification_report(y_test, y_pred))
```
:::
::::

```{python}
# Side by side: Confusion Matrix + Feature Importance
fig, axes = plt.subplots(1, 2, figsize=(12,5), dpi=140)

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(
    y_test, y_pred, cmap="Blues", ax=axes[0]
)
axes[0].set_title("Confusion Matrix — Random Forest (2024)")

# Feature Importance
importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values()
importances.plot(kind='barh', ax=axes[1], color="steelblue")
axes[1].set_title("Feature Importance — Random Forest (2024)")

plt.tight_layout()
plt.show()

```

## Q1 \| Step 4 — Evaluation

:::: tiny
We evaluate model performance with accuracy, balanced accuracy, macro-F1, a classification report, and visual diagnostics.

::: output-box
```{python}
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, f1_score,
    classification_report, confusion_matrix, ConfusionMatrixDisplay
)
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Guard: ensure we have rf, X_train, X_test, y_train, y_test from Step 3 
try:
    rf
    X_test
    y_test
except NameError as e:
    raise RuntimeError("Step 3 must run before Step 4 (train rf and create train/test splits).") from e

# Predictions
y_pred = rf.predict(X_test)

# Metrics
acc  = accuracy_score(y_test, y_pred)
bacc = balanced_accuracy_score(y_test, y_pred)
mf1  = f1_score(y_test, y_pred, average="macro")

print(f"Accuracy:           {acc:.3f}")
print(f"Balanced Accuracy:  {bacc:.3f}")
print(f"Macro F1:           {mf1:.3f}\n")

print("Classification Report:")
print(classification_report(y_test, y_pred))


```
:::
::::

```{python}
# --- Build timeline (aligned to test window) ---
timeline = pd.DataFrame(index=X_test.index)
timeline["true"] = y_test
timeline["pred"] = y_pred

# Map regimes to numeric rows and colors
order  = {"Low":0, "Medium":1, "High":2}
colors = {"Low":"green", "Medium":"orange", "High":"red"}

# Convert to numeric floats (fixes the categorical + float error)
ypos_true = timeline["true"].map(order).astype(float)
ypos_pred = timeline["pred"].map(order).astype(float)

# --- Plot: clearer rows, better legend ---
from matplotlib.lines import Line2D
fig, ax = plt.subplots(figsize=(12,4), dpi=150)

# True regimes (squares, slightly above row center)
ax.scatter(
    timeline.index, ypos_true + 0.18,
    s=42, c=timeline["true"].map(colors),
    marker='s', alpha=0.9, label="True"
)

# Predicted regimes (circles, slightly below row center)
ax.scatter(
    timeline.index, ypos_pred - 0.18,
    s=42, c=timeline["pred"].map(colors),
    marker='o', alpha=0.75, label="Predicted"
)

# y-axis formatting
ax.set_yticks([0,1,2])
ax.set_yticklabels(["Low","Medium","High"])
ax.set_ylim(-0.5, 2.5)

# faint row guides
for y in [0,1,2]:
    ax.hlines(y, xmin=timeline.index.min(), xmax=timeline.index.max(),
              colors="#dddddd", linestyles="--", linewidth=0.8, zorder=0)

# titles / labels
ax.set_title("Volatility Regimes — True vs Predicted (2024 Test Window)")
ax.set_xlabel("Date")
ax.set_ylabel("Regime")

# compact, informative legend (shape = True/Pred, color = regime)
legend_elements = [
    Line2D([0],[0], marker='s', color='w', label='True', markerfacecolor='gray', markersize=8),
    Line2D([0],[0], marker='o', color='w', label='Predicted', markerfacecolor='gray', markersize=8),
    Line2D([0],[0], marker='o', color='w', label='Low',    markerfacecolor='green',  markersize=8),
    Line2D([0],[0], marker='o', color='w', label='Medium', markerfacecolor='orange', markersize=8),
    Line2D([0],[0], marker='o', color='w', label='High',   markerfacecolor='red',    markersize=8),
]
ax.legend(handles=legend_elements, ncol=5, loc="upper center",
          bbox_to_anchor=(0.5, -0.12), frameon=False)

# clean style
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout(rect=[0,0,1,0.93])
plt.show()


```

## Q1 \| Step 5 — Insights & Takeaways

::: tiny
**Feature signals** - 20-day realized volatility and **VIX** are the strongest drivers. - **RSI** and **volume ratios** add incremental, smaller lift.

**Model performance** - Random Forest reached **balanced accuracy / macro-F1 ≈ 0.6–0.7** (holdout). - Most errors are in the **Medium** class (overlaps Low/High).

**Market interpretation** - **Low** and **High** regimes are easier to separate (calm vs. turbulence). - **Medium** reflects transition phases where signals are noisy.

**Practical lesson** - Regime classification is useful for **risk flagging** (position sizing, hedging). - The “middle ground” remains hardest to predict—consistent with trading intuition.
:::

```{python}
fig, axes = plt.subplots(1, 2, figsize=(12,4), dpi=150)

# (a) Timeline of regimes
colors = {'Low':'green','Medium':'orange','High':'red'}
for regime, color in colors.items():
    s = df_2024[df_2024['Vol_Regime'] == regime]
    axes[0].scatter(s.index, s['Volatility20'], s=12, alpha=0.7, label=regime, color=color)
axes[0].set_title("Volatility Regimes Across 2024")
axes[0].set_ylabel("20d Realized Vol (annualized)")
axes[0].legend()

# (b) Feature importances
importances = pd.Series(rf.feature_importances_, index=features).sort_values()
importances.plot(kind='barh', ax=axes[1], color="steelblue")
axes[1].set_title("Feature Importance — Random Forest")

plt.tight_layout()
plt.show()
```

## Q2 \| Hierarchical Clustering

::: smallestest
**Goal:** Organize S&P 500 stocks into a taxonomy based on persistent risk/return exposures.

**Feature design:**\
- Momentum (12-month return)\
- Volatility (20-day realized volatility)\
- Size (log market capitalization)

**Method:**\
- Construct factor-exposure matrix for each stock **(2014–2024 averages)** - Apply **hierarchical clustering (Ward’s method)**\
- Visualize dendrogram and clusters

**Validation:**\
- Stability of clusters across bootstrap samples\
- Interpretability of economic themes in groups (e.g., growth vs value, high vs low vol)

**Use:**\
- Identify **natural peer groups** of stocks\
- Aid **portfolio construction, risk diversification, sector rotation strategies**
:::

## Q2 \| Step 1 — Get Data

::: smallestest
```{python}

import yfinance as yf
import pandas as pd
import matplotlib.pyplot as plt

# Get S&P500 tickers from Wikipedia
table = pd.read_html("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies")
sp500 = table[0]

# Fix tickers for Yahoo Finance format
swap_map = {"BRK.B": "BRK-B", "BF.B": "BF-B"}
tickers = [swap_map.get(t, t) for t in sp500['Symbol'].unique().tolist()]

# Download OHLCV data (2014–2024) — not used in treemap but for consistency
data = yf.download(
    tickers, start="2014-01-01", end="2024-12-31",
    group_by="ticker", auto_adjust=True, threads=True
)

# Sector + Sub-Industry counts
sector_col = "GICS Sector"
sub_col    = "GICS Sub-Industry"

sec_counts = sp500[sector_col].value_counts().reset_index()
sec_counts.columns = ["Sector","Count"]

sub_counts = (
    sp500.groupby([sector_col, sub_col])["Symbol"]
         .count().reset_index()
         .rename(columns={sector_col:"Sector", sub_col:"SubIndustry", "Symbol":"Count"})
)

# Treemap
try:
    import squarify
except ImportError:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "squarify", "-q"])
    import squarify

import matplotlib.cm as cm
from matplotlib.patches import Patch

sectors = sec_counts["Sector"].tolist()
cmap = cm.get_cmap("tab20", len(sectors))
sector_to_color = {s: cmap(i) for i, s in enumerate(sectors)}

fig, (axL, axR) = plt.subplots(1, 2, figsize=(22,11), dpi=150)

# (A) Sector-level
squarify.plot(
    sizes=sec_counts["Count"].values,
    label=[f"{s}\n({c})" for s,c in zip(sec_counts["Sector"], sec_counts["Count"])],
    color=[sector_to_color[s] for s in sec_counts["Sector"]],
    alpha=0.95, text_kwargs={"fontsize":11}, ax=axL, pad=True
)
axL.axis("off")
axL.set_title("Sectors (size = # companies)", fontsize=14, pad=8)

# (B) Sub-industry (color by parent sector)
df_t = sub_counts[sub_counts["Count"] > 0].copy()
df_t["Label"] = df_t["SubIndustry"] + "\n(" + df_t["Count"].astype(str) + ")"
squarify.plot(
    sizes=df_t["Count"].values,
    label=df_t["Label"].values,
    color=[sector_to_color[s] for s in df_t["Sector"]],
    alpha=0.95, text_kwargs={"fontsize":7}, ax=axR, pad=True
)
axR.axis("off")
axR.set_title("Sub-Industries (color = parent sector)", fontsize=14, pad=8)

# Shared legend
handles = [Patch(facecolor=sector_to_color[s], label=s) for s in sectors]
fig.legend(handles=handles, loc="lower center", bbox_to_anchor=(0.5, 0.03),
           ncol=min(6, len(sectors)), frameon=False, fontsize=10,
           title="Sector", title_fontsize=11)

plt.tight_layout(rect=[0,0.06,1,1])
plt.show()
```
:::

## Q2 \| Step 2 — Factor Features

```{python}
#| label: q2-step2-features
#| fig-width: 16
#| fig-height: 8
#| fig-align: center
#| dpi: 150
#| out-width: "100%"

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

assert isinstance(data.columns, pd.MultiIndex), "Expected yfinance panel: columns level-0=ticker, level-1=OHLCV"
tickers = sorted(set(data.columns.get_level_values(0)))

def features_for_ticker(tk, df):
    """Compute per-ticker time series features then average 2014–2024."""
    sub = df[tk].dropna()
    if sub.shape[0] < 260:   # too little data
        return None
    ret = sub['Close'].pct_change()
    mom12 = sub['Close'] / sub['Close'].shift(252) - 1            # 12m momentum
    vol20 = ret.rolling(20).std() * np.sqrt(252)                  # 20d realized vol (annualized)
    # size proxy: log median dollar volume (robust, fast)
    dollar_vol = (sub['Close'] * sub['Volume']).replace(0, np.nan)
    size_log = np.log(dollar_vol.replace(0, np.nan)).median()

    out = pd.Series({
        'Momentum12m' : mom12.mean(skipna=True),
        'Vol20_Ann'   : vol20.mean(skipna=True),
        'Size_logDV'  : size_log
    }, name=tk)
    return out

rows = []
for tk in tickers:
    try:
        r = features_for_ticker(tk, data)
        if r is not None and r.notna().sum() == 3:
            rows.append(r)
    except Exception:
        continue

feat = pd.DataFrame(rows).dropna()
feat.shape
```

```{python}

fig, axes = plt.subplots(1,3, figsize=(16,5))
axes[0].hist(feat['Momentum12m'], bins=40)
axes[1].hist(feat['Vol20_Ann'], bins=40)
axes[2].hist(feat['Size_logDV'], bins=40)
axes[0].set_title("12m Momentum (avg 2014–2024)")
axes[1].set_title("20d Realized Vol (avg, ann.)")
axes[2].set_title("Log Dollar Volume (size proxy)")
for ax in axes:
    ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)
plt.tight_layout(); plt.show()

```

::: tiny
#### Insight

We transform raw OHLCV data into **persistent factor exposures** for each stock: - **Momentum:** 12-month return signal\
- **Volatility:** 20-day realized volatility (annualized)\
- **Size:** log of median dollar volume as a liquidity/scale proxy

This step condenses 10 years of daily data into **stable, comparable features** that form the input for clustering.
:::

## Q2 \| Step 3 — Factor Matrix & Standardize

```{python}

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

X = feat[['Momentum12m','Vol20_Ann','Size_logDV']].values
scaler = StandardScaler()
Xz = scaler.fit_transform(X)

# quick 2D view (PCA) to see structure
pca = PCA(n_components=2, random_state=42)
Z = pca.fit_transform(Xz)

fig, ax = plt.subplots(figsize=(16,6))
ax.scatter(Z[:,0], Z[:,1], s=10, alpha=0.6)
ax.set_title("PCA view of factor-exposure matrix (standardized)")
ax.set_xlabel("PC1"); ax.set_ylabel("PC2")
ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)
plt.tight_layout(); plt.show()

```

::: tiny
We convert raw features into a **standardized factor matrix**, ensuring each exposure (momentum, volatility, size) is on the same scale.\
A quick **PCA projection** reveals broad structure in the data — clusters and gradients emerge, hinting at natural groupings before formal clustering.
:::

## Q2 \| Step 4 — Hierarchical Clustering

```{python}

from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

# Ward linkage on standardized features
L = linkage(Xz, method='ward')

fig, ax = plt.subplots(figsize=(18,7))
dendrogram(L, truncate_mode='lastp', p=20, leaf_rotation=90, leaf_font_size=9, ax=ax)
ax.set_title("Hierarchical Clustering Dendrogram (Ward) — last 20 merges")
ax.set_xlabel("Merged leaves"); ax.set_ylabel("Distance")
plt.tight_layout(); plt.show()

# choose cluster count (e.g., k=6)
k = 6
labels = fcluster(L, k, criterion='maxclust')
feat['Cluster'] = labels


```

::: tiny
We apply **hierarchical clustering (Ward’s method)** on the standardized factor matrix.\
The dendrogram illustrates how stocks progressively merge into larger groups.\
By selecting **k = 6 clusters**, we capture a balance between **parsimony** (few broad groups) and **interpretability** (distinct economic themes).
:::

## Q2 \| Step 5 — Cluster Profiles & Sector Mix

```{python}
#| label: q2-step5-combined
#| fig-width: 14
#| fig-height: 6
#| fig-align: center
#| dpi: 150
#| out-width: "80%"   # shrink within slide

import numpy as np
import matplotlib.pyplot as plt

# ---- Cluster Profiles (heatmap) ----
centers = (pd.DataFrame(Xz, index=feat.index,
                        columns=['Momentum12m','Vol20_Ann','Size_logDV'])
           .assign(Cluster=feat['Cluster'])
           .groupby('Cluster').mean())

# ---- Sector Mix (% per cluster) ----
sector_col = [c for c in sp500.columns if c.lower().startswith("gics sector")][0]
ticker_to_sector = sp500.set_index('Symbol')[sector_col].to_dict()

tmp = feat[['Cluster']].copy()
tmp['Sector'] = [ticker_to_sector.get(t, 'Unknown') for t in tmp.index]

mix = (tmp.groupby(['Cluster','Sector']).size()
       .unstack(fill_value=0)
       .apply(lambda r: r/r.sum()*100, axis=1)
       .sort_index())

# ---- Side-by-side layout ----
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Left: cluster profiles heatmap
im = axes[0].imshow(centers.values, aspect='auto')
axes[0].set_xticks(range(centers.shape[1]))
axes[0].set_xticklabels(centers.columns, fontsize=10)
axes[0].set_yticks(range(centers.shape[0]))
axes[0].set_yticklabels([f"C{c}" for c in centers.index], fontsize=10)
axes[0].set_title("Cluster Profiles (z-scores)", fontsize=12)
for i in range(centers.shape[0]):
    for j in range(centers.shape[1]):
        axes[0].text(j, i, f"{centers.values[i,j]:.2f}",
                     ha='center', va='center', fontsize=8,
                     color='white' if abs(centers.values[i,j]) > 0.6 else 'black')
fig.colorbar(im, ax=axes[0], shrink=0.7, label="z-score")

# Right: sector mix stacked bar
mix.plot(kind='bar', stacked=True, ax=axes[1], width=0.85, legend=False)
axes[1].set_title("Sector Mix within Clusters (%)", fontsize=12)
axes[1].set_xlabel("Cluster", fontsize=10)
axes[1].set_ylabel("Percent", fontsize=10)
axes[1].tick_params(axis='x', rotation=0)

# Put legend outside
axes[1].legend(bbox_to_anchor=(1.05, 1), loc="upper left",
               ncol=1, frameon=False, fontsize=8, title="Sector")

plt.tight_layout()
plt.show()


```

::: tiny
This step connects **quantitative cluster features** with their **economic context**:\
- **Left (heatmap):** Each cluster shows distinct profiles in **momentum**, **volatility**, and **size**.\
- **Right (sector mix):** Clusters reveal **different sector compositions**, helping interpret groups as, e.g., “large low-vol staples” vs. “small high-momentum tech.”\
Together, these views highlight that clusters are not arbitrary — they align with **intuitive economic themes** and sector structures.
:::

## Q2 \| Step 6 — Validation (Silhouette & Bootstrap Stability)

```{python}

from sklearn.metrics import silhouette_score
from sklearn.metrics.cluster import adjusted_rand_score
from scipy.cluster.hierarchy import linkage, fcluster
import numpy as np

# Silhouette on standardized features for chosen k
sil = silhouette_score(Xz, feat['Cluster'])
print(f"Silhouette (k={feat['Cluster'].nunique()}): {sil:.3f}")

# Lightweight bootstrap stability: ARI between two bootstrap clusterings
def cluster_labels(Xz_slice, k):
    Lb = linkage(Xz_slice, method='ward')
    return fcluster(Lb, k, criterion='maxclust')

rng = np.random.default_rng(42)
idx1 = rng.choice(len(Xz), size=int(0.8*len(Xz)), replace=True)
idx2 = rng.choice(len(Xz), size=int(0.8*len(Xz)), replace=True)

lab1 = cluster_labels(Xz[idx1], k)
lab2 = cluster_labels(Xz[idx2], k)

# Compare on intersection (rough proxy)
common = min(len(lab1), len(lab2))
ari = adjusted_rand_score(lab1[:common], lab2[:common])
print(f"Bootstrap stability (ARI): {ari:.3f}")


```

:::: callout-note
::: tiny
\*\*\* Assessment \*\*\*

Separation: The silhouette score indicates moderate cluster separation—typical for financial cross-sections where factor exposures overlap.

Stability: The bootstrap ARI suggests the taxonomy is reasonably robust under resampling.

Interpretation: Clusters align with economic styles (e.g., high-momentum growth, low-volatility defensives, small/illiquid cyclicals) and often cut across GICS sectors, revealing peer groups that sector labels miss.

Actionability: Use clusters for diversification across distinct risk buckets, peer analysis, and rotation tilts. Re-validate periodically—style regimes and market microstructure can shift cluster boundaries.
:::
::::
## Q3 — LSTM for Return Forecasting



## Q3 \| Data Acquisition

Download OHLCV for `^GSPC(SPX)`, and volatility indices: `^VIX`, `^VVIX`, `^VIX9D` via `yfinance`

```{python}
"""
Notes:
    - Pull -> clean -> merge -> save df to repo for SPX/VIX 
    - auto_adjust=True returns adjusted OHLC and omits 'Adj Close'
    DataFrames for ^GSPC, ^VIX, ^VVIX, ^VIX9D with columns:
        date, OHLC and volume for SPX only (proxy)
    - Merge DataFrames (inner join on dates)
    - Saved as CSV in repo -> ./data/combined.csv
"""

from __future__ import annotations
import pandas as pd
import numpy as np
import yfinance as yf
from datetime import date
from pathlib import Path
from functools import reduce
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict
from cycler import cycler
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit
from scipy.stats import spearmanr, binomtest
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks, optimizers, Sequential
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

# Global plot settings
sns.set_theme(style="whitegrid", context="notebook", palette="deep")
plt.rcParams.update({
    "figure.dpi": 150, "savefig.dpi": 150,
    "axes.spines.top": False, "axes.spines.right": False,
    "axes.titlesize": 12, "axes.labelsize": 10,
    "legend.fontsize": 9, "xtick.labelsize": 9, "ytick.labelsize": 9,
})

'''
# Global seaborn settings
sns.set_theme(style="whitegrid", context="notebook", palette="colorblind")
plt.rcParams['figure.figsize'] = (10, 6)

SEAFOAM = "#55AA99"  # same base color as "light:#5A9"
plt.rcParams["axes.prop_cycle"] = cycler(color=[SEAFOAM])  # lines/markers default
'''

# Alias is for prefered column names.
# Start Date is for the earliest date to pull data from.
def pull_data(ticker: str, alias: str, keep_volume: bool = False) -> pd.DataFrame:
    df = yf.download(
        ticker,
        start="2014-01-01",
        end="2024-12-31",
        progress=False,  # suppress progress bar
        auto_adjust=True # no 'Adj Close'
    )

    # Check if download failed
    if df is None or df.empty:
        raise ValueError(f"Failed to download data for ticker {ticker}")

    # Flatten Index
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(0)
        df.columns.name = None   # <-- add this

    cols = ["Open", "High", "Low", "Close"] + (["Volume"] if keep_volume else [])
    df = df[cols].copy()

    # 3) Rename to lowercase + alias
    df.rename(columns={c: f"{c.lower()}_{alias}" for c in cols}, inplace=True)

    # 4) Make date a proper column
    df = df.reset_index().rename(columns={"Date": "date"})
    df["date"] = pd.to_datetime(df["date"])
    return df

# -----------------------------------------------------------------------------
# Pull Data
# -----------------------------------------------------------------------------

spx   = pull_data("^GSPC",  "spx", keep_volume=True)
vix   = pull_data("^VIX",   "vix",  keep_volume=False)
vvix  = pull_data("^VVIX",  "vvix", keep_volume=False)
vix9d = pull_data("^VIX9D", "vix9d", keep_volume=False)

for df in [spx, vix, vvix, vix9d]:
    print(df.shape)

# Column check
frames = [("spx", spx), ("vix", vix), ("vvix", vvix), ("vix9d", vix9d)]
for name, df in frames:
    print(f"\n{name} columns:")
    for c in df.columns:
        print(" ", c)

# -----------------------------------------------------------------------------
# Merge, save, and view
# -----------------------------------------------------------------------------

merged_df = reduce(lambda L, R: pd.merge(L, R, on="date", how="inner"), [spx, vix, vvix, vix9d])
merged_df = merged_df.sort_values("date").reset_index(drop=True)

Path("data").mkdir(parents=True, exist_ok=True)
merged_df.to_csv("data/merged_df.csv", index=False)



```

## 

```{python}
print("\nShape:", merged_df.shape)
print(merged_df.head())

df = merged_df.copy()
print(merged_df.info())

# Count missing values
print(merged_df.isnull().sum())
print("Total missing:", df.isnull().sum().sum())

# Show rows with any missing values
print(df[df.isnull().any(axis=1)].head())

# Check if sorted
is_chronological = df['date'].is_monotonic_increasing
print("DataFrame is in chronological order:", is_chronological)
```

## Q3 \| Feature Engineering/Standardization

::: tiny
The function `add_features` creates the following features:

-   **`log_ret`**\
    Daily log return of the index. Captures day-to-day changes in price.

-   **`ema_{span}`**\
    Exponential moving average (default 20 days). Smooths prices to show the trend.

-   **`ema_gap_pct`**\
    Difference between the price and its EMA, expressed as a percent.\
    Positive = price above trend, Negative = price below trend.

-   **`rv{window}`**\
    Realized volatility over a rolling window (default 20 days).\
    Measures how much returns fluctuate.

-   **Volatility index features** (if data is available):

    -   `{col}_pct`: Daily percent change in VIX, VIX9D, or VVIX.\
    -   `{col}_z`: How unusual the volatility level is compared to the past year.

-   **`vol_proxy`**\
    A single “volatility measure” used in modeling.

    -   Can be the 20-day realized volatility (`rv20`)\
    -   Or the VVIX index (if chosen).
:::

## Q3 \| Model Architecture LSTM

```{python}
# --- Imports
import warnings, numpy as np, pandas as pd
import matplotlib.pyplot as plt, seaborn as sns
from typing import List, Tuple
from scipy.stats import spearmanr
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix
)
import tensorflow as tf
from tensorflow.keras import layers, optimizers, Sequential

warnings.filterwarnings("ignore")
sns.set_theme(style="whitegrid")
np.random.seed(42)
tf.random.set_seed(42)
tf.get_logger().setLevel("ERROR")

# --- Config
PRICE_COL   = "close_spx"
LOOKBACK    = 60
HORIZONS    = [1, 5, 10]
TRAIN_FRAC  = 0.8

# tiny, stackable LSTM
NUM_LAYERS  = 2
UNITS       = 32
DROPOUT     = 0.20
LR          = 1e-3
EPOCHS      = 12   # adjust to 25 for fuller training
BATCH       = 64

# --- Data (fallback synthetic if df_raw not already defined in the session)
if "df_raw" not in globals():
    T = 1600
    dates = pd.bdate_range("2018-01-01", periods=T, freq="B")
    ret = np.random.normal(0.0002, 0.007, T)
    close_spx = 3000.0 * np.exp(np.cumsum(ret))
    noise = np.random.normal(0, 0.4, T).cumsum()/20
    base_vix = 18 + 1.5*np.sin(np.linspace(0, 14, T)) + noise
    close_vix = np.clip(base_vix, 10, 60)
    close_vix9d = np.clip(base_vix + 0.6*np.sin(np.linspace(0, 28, T)+0.7) + np.random.normal(0,0.6,T), 10, 60)
    close_vvix = np.clip(95 + np.random.normal(0, 1.2, T).cumsum()/10 + 0.3*np.arange(T)/T*20, 60, 220)

    df_raw = pd.DataFrame({
        "date": dates, "close_spx": close_spx,
        "close_vix": close_vix, "close_vix9d": close_vix9d, "close_vvix": close_vvix
    })

# --- Feature builder (returns ONLY features to avoid column overlap on join)
def make_features(
    df: pd.DataFrame,
    date_col: str | None = "date",
    close_col: str = "close_spx",
    vix_col: str = "close_vix",
    vix9d_col: str = "close_vix9d",
    vvix_col: str = "close_vvix",
    ema_span: int = 20,
    rv_window: int = 20,
    use_vvix_spread: bool = False,     # False => use rv20; True => use (VVIX/VIX - 1)
    add_multi_returns: bool = True,    # adds r1, r5, r21
    horizons: List[int] = [1, 5, 21],
    add_booleans: bool = False         # adds aboveEMA20, ts_inverted
) -> Tuple[pd.DataFrame, List[str]]:
    out = df.copy()
    if date_col and date_col in out.columns:
        out[date_col] = pd.to_datetime(out[date_col], errors="coerce")
        out = out.dropna(subset=[date_col]).sort_values(date_col).set_index(date_col)
    else:
        out = out.sort_index()

    req = [close_col, vix_col, vix9d_col]
    if use_vvix_spread:
        req.append(vvix_col)
    for c in req:
        if c not in out.columns:
            raise KeyError(f"Missing required column: {c}")
        out[c] = pd.to_numeric(out[c], errors="coerce")

    # Returns
    out["r1"] = out[close_col].pct_change(1)
    if add_multi_returns:
        for h in horizons:
            out[f"r{h}"] = out[close_col].pct_change(h)

    # EMA and transforms
    ema20 = out[close_col].ewm(span=ema_span, adjust=False, min_periods=ema_span).mean()
    out["ema_dist_20_pct"]  = out[close_col] / ema20 - 1
    out["ema_slope_20_pct"] = ema20.pct_change()

    # Volatility feature
    if use_vvix_spread:
        out["vvix_over_vix_minus1"] = out[vvix_col] / out[vix_col] - 1
        vol_col = "vvix_over_vix_minus1"
    else:
        minp = max(5, rv_window // 2)
        out["rv20"] = out["r1"].rolling(rv_window, min_periods=minp).std()
        vol_col = "rv20"

    # Term structure
    out["vix9d_over_vix_minus1"] = out[vix9d_col] / out[vix_col] - 1

    # Optional booleans (shifted by 1 bar to avoid leakage)
    bool_cols = []
    if add_booleans:
        out["aboveEMA20"]  = (out[close_col] > ema20).shift(1).astype("Int8")
        out["ts_inverted"] = (out[vix9d_col] > out[vix_col]).shift(1).astype("Int8")
        bool_cols = ["aboveEMA20", "ts_inverted"]

    base_cols = ["r1", "ema_dist_20_pct", "ema_slope_20_pct", "vix9d_over_vix_minus1", vol_col]
    if add_multi_returns:
        extra = [f"r{h}" for h in horizons if h != 1]
        base_cols += extra
    feat_cols = base_cols + bool_cols

    # ✅ Return ONLY feature columns (prevents overlap with PRICE_COL on join)
    features_df = out[feat_cols].dropna().copy()
    return features_df, feat_cols

# --- Windowing / scaling / model / metrics / plots
def make_windows_arrays(X_scaled, y, lookback: int, horizon: int):
    Xw, yw = [], []
    T = len(X_scaled)
    for t in range(lookback, T - horizon + 1):
        Xw.append(X_scaled[t - lookback:t, :])
        yw.append(y[t + horizon - 1])
    return np.asarray(Xw, "float32"), np.asarray(yw)

def split_scale_window(base: pd.DataFrame, feat_cols, H: int):
    y_bin = (np.log(base[PRICE_COL]).shift(-H) - np.log(base[PRICE_COL]) > 0).astype("int8")
    data = base.dropna(subset=feat_cols + [PRICE_COL]).join(y_bin.rename("y_bin")).dropna()
    X_all = data[feat_cols].astype("float32").to_numpy()
    y_all = data["y_bin"].to_numpy()

    n = len(X_all); n_train = int(n * TRAIN_FRAC)
    scaler = StandardScaler().fit(X_all[:n_train])
    X_tr_block = scaler.transform(X_all[:n_train])
    X_te_block = scaler.transform(X_all[n_train - LOOKBACK:])
    y_tr_block = y_all[:n_train]
    y_te_block = y_all[n_train - LOOKBACK:]

    Xw_tr, yw_tr = make_windows_arrays(X_tr_block, y_tr_block, LOOKBACK, H)
    Xw_te, yw_te = make_windows_arrays(X_te_block, y_te_block, LOOKBACK, H)

    n_tr = len(Xw_tr); n_val = max(1, int(0.2 * n_tr))
    X_tr_core, y_tr_core = Xw_tr[:-n_val], yw_tr[:-n_val]
    X_val,     y_val     = Xw_tr[-n_val:],  yw_tr[-n_val:]
    return X_tr_core, y_tr_core, X_val, y_val, Xw_te, yw_te

def build_lstm_classifier(timesteps: int, n_features: int) -> tf.keras.Model:
    m = Sequential()
    for i in range(NUM_LAYERS):
        m.add(layers.LSTM(UNITS, return_sequences=(i < NUM_LAYERS - 1),
                          input_shape=None if i else (timesteps, n_features)))
        if DROPOUT:
            m.add(layers.Dropout(DROPOUT))
    m.add(layers.Dense(1, activation="sigmoid"))
    m.compile(optimizer=optimizers.Adam(learning_rate=LR),
              loss="binary_crossentropy", metrics=["accuracy"])
    return m

def class_weights_from_labels(y: np.ndarray) -> dict:
    classes = np.array([0, 1])
    w = compute_class_weight(class_weight="balanced", classes=classes, y=y)
    return {0: float(w[0]), 1: float(w[1])}

def tune_threshold(y_val_true: np.ndarray, y_val_prob: np.ndarray, metric: str = "balacc"):
    ts = np.linspace(0.05, 0.95, 37)
    if metric == "balacc":
        scores = [balanced_accuracy_score(y_val_true, (y_val_prob >= t).astype(int)) for t in ts]
    else:
        scores = [f1_score(y_val_true, (y_val_prob >= t).astype(int)) for t in ts]
    t_best = float(ts[int(np.argmax(scores))])
    return t_best, pd.DataFrame({"tau": ts, "score": scores})

def score_table(y_true, y_prob, tau):
    y_true = np.asarray(y_true).astype(int)
    y_hat  = (np.asarray(y_prob) >= tau).astype(int)
    return {
        "Acc":      accuracy_score(y_true, y_hat),
        "BalAcc":   balanced_accuracy_score(y_true, y_hat),
        "F1":       f1_score(y_true, y_hat),
        "ROC_AUC":  roc_auc_score(y_true, y_prob),
        "PR_AUC":   average_precision_score(y_true, y_prob),
        "IC":       spearmanr(y_true, y_prob).correlation,
        "tau":      float(tau),
    }

def plot_learning_curves(history, title=""):
    h = history.history if hasattr(history, "history") else history
    df = pd.DataFrame({"epoch": np.arange(1, len(h["loss"])+1),
                       "train": h["loss"],
                       "val":   h.get("val_loss", [np.nan]*len(h["loss"]))})
    dm = df.melt("epoch", var_name="split", value_name="loss")
    plt.figure(figsize=(6,3))
    ax = sns.lineplot(data=dm, x="epoch", y="loss", hue="split", marker="o")
    ax.set(title=f"Learning curves {title}", xlabel="Epoch", ylabel="Loss"); ax.legend(title="")
    plt.tight_layout(); plt.show()

def plot_roc_pr(y_true, y_prob, title=""):
    from sklearn.metrics import roc_curve, precision_recall_curve
    fig, ax = plt.subplots(1,2, figsize=(9,3))
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    ax[0].plot(fpr, tpr); ax[0].plot([0,1],[0,1],"k--",lw=1)
    ax[0].set(title=f"ROC AUC={roc_auc_score(y_true,y_prob):.3f} {title}",
              xlabel="FPR", ylabel="TPR")
    prec, rec, _ = precision_recall_curve(y_true, y_prob)
    ax[1].plot(rec, prec)
    ax[1].set(title=f"PR AUC={average_precision_score(y_true,y_prob):.3f} {title}",
              xlabel="Recall", ylabel="Precision")
    plt.tight_layout(); plt.show()

def plot_confusion_at_tau(y_true, y_prob, tau, title=""):
    cm = confusion_matrix(y_true, (y_prob >= tau).astype(int), labels=[0,1])
    fig, ax = plt.subplots(figsize=(3.6,3.2))
    sns.heatmap(cm, annot=True, fmt="d", cbar=False, cmap="Blues", ax=ax)
    ax.set(title=f"Confusion (τ={tau:.2f}) {title}", xlabel="Pred", ylabel="True")
    plt.tight_layout(); plt.show()

def plot_prob_hist(y_prob, title=""):
    plt.figure(figsize=(6,3))
    sns.histplot(y_prob, bins=40, kde=True)
    plt.axvline(0.5, color="k", ls="--", lw=1)
    plt.title(title or "Score distribution (test)"); plt.xlabel("P(Up)")
    plt.tight_layout(); plt.show()

# --- Build features and safe-join the price (no overlap)
features_df, feat_cols = make_features(
    df=df_raw,
    date_col="date",
    close_col="close_spx",
    vix_col="close_vix",
    vix9d_col="close_vix9d",
    vvix_col="close_vvix",
    ema_span=20,
    rv_window=20,
    use_vvix_spread=False,
    add_multi_returns=True,
    horizons=[1,5,21],
    add_booleans=False
)

assert PRICE_COL not in features_df.columns, "features_df must not include price column"
df_prices = df_raw.set_index("date")[[PRICE_COL]]
base = features_df.join(df_prices, how="inner")

# print("Features:", feat_cols)
# print("Rows:", len(base), "| Span:", base.index.min().date(), "→", base.index.max().date())



```
::: {layout-ncol=2}
![](images/1.png){alt="Left image" height=65%}
![](images/1.png){alt="Left image" height=65%}
::: 

##
::: {layout-ncol=2}
![](images/3.png)
:::
##
::: {layout-ncol=2}
![](images/4.png){alt="Left image" height=65%}

:::

##
::: {layout-ncol=2}
![](images/5.png){alt="Left image" height=65%}

:::


## Q3 \| Evaluation Metrics MAE and RMSE for each forecast horizon

::: tiny
**Target horizons:** **Short (1d)**, **Medium (5d)**, **Long (21d)** returns (log or pct).

**Inputs:** OHLCV, features

**Setup:** Walk-forward or expanding window; scale features **within train folds** only.

**Baselines:** Naïve (random walk), **ARIMA**, linear reg, tree ensembles.

**Metrics:** **MAE / RMSE**, **R²**, **directional accuracy**, **IC (Spearman)** for ranked forecasts.

**Outputs:** Learning curves, forecast vs. actual, error distribution, feature ablations.
:::

:::: callout-note
::: tiny
\*\*\* Assessment \*\*\*

***Q3 — LSTM for Return Forecasting***

Model has changed! this is place holder
:::
::::

## Q4 — Accuracy vs. Prediction Horizon

::: smaller
**Question:** How does error grow as horizon increases—and what does that imply?

**Procedure:** - Train identical pipelines across horizons (1, 5, 21) - Record **MAE/RMSE**, **directional hit-rate**, **IC** per horizon - Plot **error(horizon)** and **hit-rate(horizon)** curves with CIs

**Interpretation:** - Information decays with horizon → **bias ↑**, **variance ↓/↑** (model-dependent) - Longer horizons may need **richer features** (macro/term structure) or **different models** - Practical takeaways for **portfolio holding periods** and **signal half-life**
:::

:::: callout-note
::: tiny
\*\*\* Assessment \*\*\* ***Q4 — Accuracy vs. Prediction Horizon***

Model has changed! this is place holder
:::
::::