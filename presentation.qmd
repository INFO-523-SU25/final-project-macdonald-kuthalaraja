---
title: "Navigating the S&P 500 with Machine Learning"
subtitle: "INFO 523 - Summer 2025 - Final Project"
author: "Trevor Macdonald & Nandakumar Kuthalaraja"
title-slide-attributes:
  data-background-image: images/sp500.png
  data-background-size: cover
  data-background-opacity: ".4"
  data-slide-number: none
  style: "color: black;"
format:
  revealjs:
    theme:  ['data/customtheming.scss']
    css: images/style.css
  
editor: visual
jupyter: python3
execute:
  echo: false
---

## What is the S&P 500?

:::{.smaller}
- The **S&P 500 (Standard & Poor’s 500 Index)** is a stock market index that tracks the performance of **500 of the largest publicly traded companies** in the United States.  
- It is considered a **benchmark** for the overall U.S. stock market and economy.  
- The index covers companies across **11 sectors**, including technology, healthcare, finance, and consumer goods. 
- Investors, analysts, and researchers use the S&P 500 to:  
  - Measure market performance  
  - Benchmark investments  
  - Study trends in the U.S. economy  
::: 


## Distribution of S&P 500

:::{.smaller}
```{python}
#| label: setup001

import pandas as pd
import matplotlib.pyplot as plt

# 1) Load current S&P 500 constituents with sectors from Wikipedia
url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
dfs = pd.read_html(url, match="Symbol")
df = dfs[0].copy()

# 2) Normalize column names and get sector column robustly
df.columns = [c.strip() for c in df.columns]
sector_col = [c for c in df.columns if c.lower().startswith("gics sector")][0]

# 3) Count & percentage by sector
counts = df[sector_col].value_counts().sort_values(ascending=True)
pct = (counts / counts.sum() * 100).round(1)

# 4) Plot (horizontal bar chart with labels)
fig, ax = plt.subplots(figsize=(10,6), dpi=150)
ax.barh(counts.index, pct.values)

for i, (v) in enumerate(pct.values):
    ax.text(v, i, f"  {v:.1f}%", va="center")

ax.set_xlabel("Percent of Companies")
ax.set_ylabel("")
ax.set_title("S&P 500 Companies by Sector (%)", pad=12)
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)
plt.tight_layout()
plt.show()

```

::: 

## Our Research Questions (Overview)

:::{.smaller}
- **Q1:** Classify each 2024 trading day into **low / medium / high volatility** from recent price action & indicators.
- **Q2:** Use **hierarchical clustering** to build a taxonomy of S&P 500 stocks from **multi-factor risk/return** features.
- **Q3:** Train an **LSTM** to forecast **short / medium / long**-horizon S&P 500 returns.
- **Q4:** Examine how **forecast accuracy degrades** with horizon—and what that implies about modeling **longer-term trends**.
:::


## Q1 — Volatility Regime Classification

:::{.smaller}
**Goal:** Classify each 2024 trading day into **Low / Medium / High volatility** regimes.  

**Why it matters:** Knowing the regime helps with **risk control, hedging, and position sizing**.  

**Features (look-back window):**
- Realized volatility (rolling std of returns), intraday range  
- Momentum & trend (moving averages, RSI)  
- Volume dynamics (volume ratios)  
- Market sentiment (VIX)  

**Labels:** Split realized volatility into **tertiles** (low / medium / high).  

**Models tested:** Logistic Regression, Random Forest, XGBoost  
**Evaluation:** Macro-F1, balanced accuracy, confusion matrix  
:::


## Q1 | Step 1 — Feature Engineering
:::{.smallest}

We construct key price-based and market indicators:

- **20-day rolling volatility**  
- **Intraday range** (High – Low) / Close  
- **Moving averages** (MA10 / MA50 ratio)  
- **Volume ratios** (Volume / MA20)  
- **RSI** (14-day)  
- **VIX index** (implied volatility)  

These features summarize short-term market dynamics and investor sentiment.
:::
```{python}
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- helper to normalize columns ---
def normalize_columns(df):
    if isinstance(df.columns, pd.MultiIndex):
        df = df.copy()
        df.columns = df.columns.get_level_values(0)
    else:
        # handle stringified tuples like "('Close','^GSPC')"
        import ast
        new_cols = []
        for c in df.columns:
            s = str(c)
            if s.startswith("(") and "," in s:
                try:
                    t = ast.literal_eval(s)
                    new_cols.append(t[0])
                except Exception:
                    new_cols.append(s)
            else:
                new_cols.append(s)
        df = df.copy()
        df.columns = new_cols
    return df

# --- download GSPC ---
data = yf.download('^GSPC', start='2014-01-01', end='2024-12-31', auto_adjust=False)
data = normalize_columns(data)

# --- download VIX ---
vix = yf.download('^VIX', start='2014-01-01', end='2024-12-31', auto_adjust=False)
vix = normalize_columns(vix)
vix = vix[['Close']].rename(columns={'Close':'VIX'})

# --- normalize indexes ---
data.index = pd.to_datetime(data.index).tz_localize(None)
vix.index   = pd.to_datetime(vix.index).tz_localize(None)

# --- join safely ---
data = data.join(vix, how='left')
data['VIX'] = data['VIX'].ffill()

# features needed for the plot
data['Return'] = data['Close'].pct_change()
data['Volatility20'] = data['Return'].rolling(20, min_periods=20).std() * np.sqrt(252)

subset = data.loc['2024', ['Volatility20','VIX']].dropna(how='all')
fig, ax = plt.subplots(figsize=(10,5), dpi=150)
subset['Volatility20'].plot(ax=ax, label='20d Realized Vol')
subset['VIX'].plot(ax=ax, label='VIX', alpha=0.7)
ax.set_title("2024: Realized vs Implied Volatility")
ax.legend()
plt.tight_layout()
plt.show()


```

## Q1 | Step 2 — Target Creation
  
:::{.smallest}
We label each 2024 trading day as **Low / Medium / High** using tertiles of realized volatility, and visualize the assignments across the year.
:::
```{python}
import numpy as np
import matplotlib.pyplot as plt

# ---- Guard: create targets if not already present ----
if 'df_2024' not in globals() or 'Vol_Regime' not in df_2024.columns:
    # Build from Step 1 outputs
    data['Return'] = data['Close'].pct_change()
    data['Volatility20'] = data['Return'].rolling(20, min_periods=20).std() * np.sqrt(252)
    df_2024 = data.loc['2024'].dropna(subset=['Volatility20']).copy()
    low_q, high_q = df_2024['Return'].abs().quantile([0.33, 0.66])
    df_2024['Vol_Regime'] = pd.cut(
        df_2024['Return'].abs(),
        bins=[-np.inf, low_q, high_q, np.inf],
        labels=['Low','Medium','High']
    )

# --- Colors for regimes ---
colors = {"Low": "green", "Medium": "orange", "High": "red"}

# --- Create combined figure ---
fig, axes = plt.subplots(1, 2, figsize=(14,5), dpi=150)

# (A) Volatility timeline with regimes
ax = axes[0]
for regime, color in colors.items():
    s = df_2024[df_2024['Vol_Regime'] == regime]
    ax.scatter(s.index, s['Volatility20'], s=15, alpha=0.7, color=color, label=regime)
ax.set_title("Volatility Regimes Across 2024")
ax.set_ylabel("20d Realized Vol (annualized)")
ax.set_xlabel("Date")

# (B) Closing price with regimes
ax = axes[1]
ax.plot(df_2024.index, df_2024['Close'], color='black', linewidth=1, alpha=0.8)
for regime, color in colors.items():
    mask = df_2024['Vol_Regime'] == regime
    ax.scatter(df_2024.index[mask], df_2024['Close'][mask], s=12, c=color, alpha=0.8)
ax.set_title("S&P 500 Closing Price with Regimes (2024)")
ax.set_xlabel("Date")
ax.set_ylabel("Index Level")

# --- Shared legend ---
handles = [plt.Line2D([0],[0], marker='o', color='w', label=k,
                      markerfacecolor=v, markersize=6) for k, v in colors.items()]
fig.legend(handles=handles, labels=list(colors.keys()),
           loc='upper center', ncol=3, frameon=False)

plt.tight_layout(rect=[0,0,1,0.94])
plt.show()

```
::::{.smallest}
:::{.callout-note}
Balanced Labels — Days are evenly split into Low, Medium, High volatility using realized vol tertiles.

Temporal Patterns — High-volatility clusters appear during market drawdowns, while low regimes align with calm uptrends.

Practical Use — These labels provide the supervised learning target for Step 3 (model training).
:::
::::

## Q1 | Step 3 — Model Training
::::{.smallest}
We train a **Random Forest classifier** on engineered features to predict volatility regimes.  
Evaluation is done with a confusion matrix and feature importance ranking.


```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import ConfusionMatrixDisplay, classification_report

# 0) Ensure VIX exists (must have been joined in Step 1)
assert 'VIX' in data.columns, "VIX missing on `data`. Run Step 1 cell that joins VIX."

# 1) Recompute features on the full `data` to be safe
data = data.copy()
data['Return'] = data['Close'].pct_change()
data['Volatility20'] = data['Return'].rolling(20, min_periods=20).std() * np.sqrt(252)
data['Range'] = (data['High'] - data['Low']) / data['Close']
data['MA10'] = data['Close'].rolling(10, min_periods=10).mean()
data['MA50'] = data['Close'].rolling(50, min_periods=50).mean()
data['MA_ratio'] = data['MA10'] / data['MA50']
data['Vol_MA20'] = data['Volume'].rolling(20, min_periods=5).mean()
data['Vol_ratio'] = data['Volume'] / data['Vol_MA20']

# 2) Build 2024 frame + target labels
df_2024 = data.loc['2024'].copy()
df_2024['DailyVol'] = df_2024['Return'].abs()

q1, q2 = df_2024['DailyVol'].quantile([0.33, 0.66])
df_2024['Vol_Regime'] = pd.cut(
    df_2024['DailyVol'],
    bins=[-np.inf, q1, q2, np.inf],
    labels=['Low','Medium','High']
)

# 3) Feature matrix (fill gaps so nothing is missing in early 2024 windows)
features = ['Volatility20','Range','MA_ratio','Vol_ratio','RSI','VIX']

# If RSI wasn't computed in this session, create it
if 'RSI' not in df_2024.columns:
    delta = data['Close'].diff()
    up = delta.clip(lower=0)
    down = (-delta).clip(lower=0)
    roll_up = up.rolling(14, min_periods=14).mean()
    roll_down = down.rolling(14, min_periods=14).mean().replace(0, np.nan)
    RS = roll_up / roll_down
    data['RSI'] = 100 - (100 / (1 + RS))
    df_2024['RSI'] = data.loc[df_2024.index, 'RSI']

# Make sure all features exist now
missing = [c for c in features if c not in df_2024.columns]
if missing:
    raise ValueError(f"Missing features in df_2024: {missing}")

# Fill NaNs from rolling windows
df_2024[features] = df_2024[features].bfill().ffill()

# Drop any remaining NaNs along features/target
df_2024 = df_2024.dropna(subset=features + ['Vol_Regime'])

X = df_2024[features]
y = df_2024['Vol_Regime']

# 4) Time-aware split (no shuffling)
split_idx = int(len(df_2024) * 0.75)
X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

# 5) Train model
rf = RandomForestClassifier(n_estimators=300, random_state=42)
rf.fit(X_train, y_train)

# 6) Evaluate
y_pred = rf.predict(X_test)
print(classification_report(y_test, y_pred))
```
::::

```{python}
# --- Side by side: Confusion Matrix + Feature Importance ---
fig, axes = plt.subplots(1, 2, figsize=(12,5), dpi=140)

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(
    y_test, y_pred, cmap="Blues", ax=axes[0]
)
axes[0].set_title("Confusion Matrix — Random Forest (2024)")

# Feature Importance
importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values()
importances.plot(kind='barh', ax=axes[1], color="steelblue")
axes[1].set_title("Feature Importance — Random Forest")

plt.tight_layout()
plt.show()

```

## Q1 | Step 4 — Evaluation
::::{.smallest}
We evaluate model performance with accuracy, balanced accuracy, macro-F1, a classification report, and visual diagnostics.


```{python}
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, f1_score,
    classification_report, confusion_matrix, ConfusionMatrixDisplay
)
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# ---- Guard: ensure we have rf, X_train, X_test, y_train, y_test from Step 3 ----
try:
    rf
    X_test
    y_test
except NameError as e:
    raise RuntimeError("Step 3 must run before Step 4 (train rf and create train/test splits).") from e

# Predictions
y_pred = rf.predict(X_test)

# Metrics
acc  = accuracy_score(y_test, y_pred)
bacc = balanced_accuracy_score(y_test, y_pred)
mf1  = f1_score(y_test, y_pred, average="macro")

print(f"Accuracy:           {acc:.3f}")
print(f"Balanced Accuracy:  {bacc:.3f}")
print(f"Macro F1:           {mf1:.3f}\n")

print("Classification Report:")
print(classification_report(y_test, y_pred))


```
::::
```{python}

# ---- Visual 2: Timeline strip — True vs Predicted Regimes on the 2024 test window ----
# Build a small timeline frame aligned to X_test indices
timeline = pd.DataFrame(index=X_test.index)
timeline["true"] = y_test
timeline["pred"] = y_pred

# Map regimes to y positions and colors
order  = {"Low":0, "Medium":1, "High":2}
colors = {"Low":"green", "Medium":"orange", "High":"red"}
ypos_true = timeline["true"].map(order)
ypos_pred = timeline["pred"].map(order)

fig, ax = plt.subplots(figsize=(12,3.6), dpi=150)

# True (upper row)
ax.scatter(timeline.index, ypos_true, s=18,
           c=timeline["true"].map(colors), marker='s', label='True', alpha=0.9)

# Pred (lower row, offset for readability)
ax.scatter(timeline.index, ypos_pred - 0.15, s=18,
           c=timeline["pred"].map(colors), marker='o', label='Pred', alpha=0.7)

ax.set_yticks([0,1,2])
ax.set_yticklabels(["Low","Medium","High"])
ax.set_title("Volatility Regimes — True vs Predicted (2024 test window)")
ax.set_xlabel("Date")
ax.set_ylabel("Regime")
ax.legend(loc="upper left", ncol=2, frameon=False)
ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)
plt.tight_layout()
plt.show()

```



## Q1 | Step 5 — Insights & Takeaways

::::{.smallest}
- **Feature Signals**
  - 20-day realized volatility and VIX are the strongest drivers of regime classification.
  - RSI and volume ratios provide incremental but smaller predictive power.  

- **Model Performance**
  - Random Forest achieved balanced accuracy and macro-F1 in the ~0.6–0.7 range.  
  - Most errors occurred in **Medium regime**, which overlaps characteristics of Low and High.  

- **Market Interpretation**
  - Low and High regimes are easier to separate — extreme calm vs turbulence.  
  - Medium regime reflects transition phases where market signals are noisy.  

- **Practical Lesson**
  - Classification can flag periods of *elevated vs subdued* risk effectively.  
  - Predicting the “middle ground” remains the hardest task — consistent with real trading experience.  
::::
```{python}
fig, axes = plt.subplots(1, 2, figsize=(12,4), dpi=150)

# (a) Timeline of regimes
colors = {'Low':'green','Medium':'orange','High':'red'}
for regime, color in colors.items():
    s = df_2024[df_2024['Vol_Regime'] == regime]
    axes[0].scatter(s.index, s['Volatility20'], s=12, alpha=0.7, label=regime, color=color)
axes[0].set_title("Volatility Regimes Across 2024")
axes[0].set_ylabel("20d Realized Vol (annualized)")
axes[0].legend()

# (b) Feature importances
importances = pd.Series(rf.feature_importances_, index=features).sort_values()
importances.plot(kind='barh', ax=axes[1], color="steelblue")
axes[1].set_title("Feature Importance — Random Forest")

plt.tight_layout()
plt.show()
```


## Q2 — Hierarchical Clustering of Stocks

:::{.smaller}
**Goal:** Organize S&P 500 into a **data-driven taxonomy**.

**Feature design options:**
- **Risk/return** factors (e.g., momentum, value, size, quality, low-vol)
- **Return correlations** over rolling windows
- **Volatility / beta / drawdown** statistics

**Method:** Choose a distance (e.g., **1–ρ**, Euclidean on factor z-scores), linkage (**Ward**, **average**).  
Produce **dendrogram** + cluster cuts (k ≈ 6–12).

**Validation:** **Silhouette**, **cophenetic correlation**, cluster **stability** across windows.

**Use:** Diversification views, **peer groups**, sector vs. data-driven comparisons.
:::


## Q3 — LSTM for Return Forecasting

:::{.smaller}
**Target horizons:** **Short (1–5d)**, **Medium (10–20d)**, **Long (60–120d)** returns (log or pct).

**Inputs:** OHLCV, technicals (momentum/vol/overbought), rolling stats, optional macro proxies.

**Setup:** Walk-forward or expanding window; scale features **within train folds** only.

**Baselines:** Naïve (random walk), **ARIMA**, linear reg, tree ensembles.

**Metrics:** **MAE / RMSE**, **R²**, **directional accuracy**, **IC (Spearman)** for ranked forecasts.

**Outputs:** Learning curves, forecast vs. actual, error distribution, feature ablations.
:::



## Q4 — Accuracy vs. Prediction Horizon

:::{.smaller}
**Question:** How does error grow as horizon increases—and what does that imply?

**Procedure:**
- Train identical pipelines across horizons (1, 5, 10, 20, 60, 120d)
- Record **MAE/RMSE**, **directional hit-rate**, **IC** per horizon
- Plot **error(horizon)** and **hit-rate(horizon)** curves with CIs

**Interpretation:**
- Information decays with horizon → **bias ↑**, **variance ↓/↑** (model-dependent)
- Longer horizons may need **richer features** (macro/term structure) or **different models**
- Practical takeaways for **portfolio holding periods** and **signal half-life**
:::